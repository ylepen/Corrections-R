---
title: "Modélisation du taux de chômage"
author: "Yannick Le Pen"
date: "2023-2024"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
```


### Importation de la série du taux de chômage à partir de DB.nomics

On importe les observations trimestrielles du taux de chômage aux Etats-Unis du 1994:T1 au 23:T1 (117 observations) à partir du site [DB.nomics](https://db.nomics.world/BLS ). Les données proviennent du [Bureau of Labor Statistics](https://www.bls.gov/).

```{r}
rm(list = ls())
library(data.table)
library(rdbnomics)
df<-rdb(ids = "BLS/ln/LNS13327708Q") # ids = identifiant de la série sur DB.nomics
```

On crée un dataframe contenant uniquement les dates (colonne "period") et la série du taux de chômage (colonne "value").

```{r}
df_u<-df[,c("period","value")]
colnames(df_u)<-c('date','taux')
head(df_u)
tail(df_u,n=5)
```
#### Représentation graphique

Le taux de chômage aux Etats-Unis fluctue autour de 6% du début de l'échantillon jusqu'à l'année 2008 à partir de laquelle il augmente très rapidement jusqu'au niveau de 11%, sous l'effet de la crise des subprimes. On observe aussi l'épisode de la crise du Covid avec une hausse instantanée du taux de chômage autour de 14% suivie d'un retour rapide à des niveaux très bas. La période du covid peut avoir un impact sur les résultats des tests statistiques. Au vu du graphique, il est assez difficile de trancher sur la stationnarité du taux de chômage.

```{r}
library(ggplot2)
p_us<-ggplot(data=df_u,aes(x=date,y=taux))+geom_point(color='blue')+geom_line(color='blue')
p_us+labs(x="date",y="taux",title = "Taux de chômage aux Etats-Unis",subtitle = "1994:T1 à 2023:T1")
```


### Estimation sans la période Covid : 1994:T1 à 2019:T4

Dans un premier temps, on exclut les observations à partir du dernier trimestre de l'année 2019. 

```{r}
df_u_19T1=df_u[df_u$date<="2019-10-01",]
```

#### Représentation graphique


```{r}
library(ggplot2)
p_us<-ggplot(data=df_u_19T1,aes(x=date,y=taux))+geom_point(color='blue')+geom_line(color='blue')
p_us+labs(x="date",y="taux",title = "Taux de chômage aux Etats-Unis",subtitle = "1994:T1 à 2019:T4")
```

Le graphique montre que pendant la grande récession de 2008-2009, le taux de chômage a atteint des niveaux nettement plus élevés que depuis le début de l'échantillon. 


#### Statistiques descriptives et autocorrélogrammes

```{r}
statdes0<-summary(df_u_19T1$taux)
sprintf("Ecart type %f",sd(df_u_19T1$taux))
```

La library FinTS permet aussi de calculer des statistiques descriptives d'une série temporelle.
```{r}
library(FinTS)
stat_des1<-FinTS.stats(df_u_19T1$taux)
stat_des1
```


`


```{r}
acf_u_19T1<-acf(x=df_u_19T1$taux,main='Autocorrélogramme du taux de chômage - 1994T1-2019T4')
pacf(x=df_u_19T1$taux,main='Autocorrélogramme partiel du taux de chômage - 1994T1-2019T4')
```


**Commentaires**

* Les autocorrélations sont positives et significativement jusqu'au retard 11.
* Elles sont proches de 1 pour les retards 1, 2, 3, 4 5 puis décroissent vers 0. On en déduit que le taux de chômage est assez autocorrélé. Néanmoins, il est assez difficile de tirer une conclusions sur la stationnarité ou non-stationnarité du taux de chômage. le profil des autocorrélations pourrait correspondre à une série stationnaire. 
* Les autocorrélations partielles sont significatives pour les retards 1, 2 et 3.

Le profil des autocorrélations suggère d'esimer un modèle AR(3)


### Estimation et tests de validation d'un modèle ARMA

On va écarter les 4 dernières observations de l'échantillon, c'est-à-dire l'année 2019. Dans notre cas, cela revient à estimer le modèle avec les 100 premières observations. L'objectif est de comparer les valeurs prédites par le modèle final aux valeurs réalisées. Ainsi, on pourra évaluer la qualité des prévisions en-dehors de l'échantillon ("out-of-sample) utilisé pour l'estimer.


On utilise la librairie forecast. En se basant sur l'analyse des autocorrélations, on spécifie un modèle AR(3) :

* order=c(p,d,q) où :

    * p : nombre de retards de la partie AR, 
    * d : nombre de différenciations pour obtenir un série stationnaire, 
    * q : nombre de retards pour la partie MA
    
* Arima inclut une constante par défaut

```{r}
library(forecast)
# Estimation d'un  modèle 
AR3<-Arima(y = df_u_19T1$taux[1:100],order=c(3,0,0))
# Affichage de l'estimation
summary(AR3) 
``` 



```{r}
library(ggplot2)
library(reshape2)

df_plot<-data.frame(df_u_19T1$date[1:100],df_u_19T1$taux[1:100],F=as.matrix(AR3$fitted))
colnames(df_plot)<-c("date","observé","ajusté")

data_melt=melt(as.data.frame(df_plot),id.vars = 1)

p_F_us<-ggplot(data=data_melt,aes(x=date))+geom_line(aes(x=date,y=value,color= variable))
p_F_us+labs(y="taux en pourcentage",title = "Valeur observée et valeur ajustée du taux de chômage",
            subtitle = "1994:T1 à 2018:T4")
```

#### Test de significativité des coefficients

1. Avec la librairie lmtest


```{r}
library(lmtest)
coeftest(AR3)
```
**Commentaires**

1. On vérifie la significativité de la constante : 

$\left\lbrace\begin{array}{l} H_{0}\,:\,c=0\\
H_{a}\,:\,c \neq 0
\end{array} \right.$ 

* La statistique de test $t_{\hat{c}} = \frac{\hat{c}}{\hat{\sigma}_{\hat{c}}} \rightarrow N(0,1)$ sous $H_{0}$ car On dispose de 104 observations.
* Les seuils critiques sont :
*       seuil = 2.57 pour un risque de première espèce égal à 1\% 
*       seuil = 1.96 pour un risque de première espèce égal à 5\% 

* $t_{\hat{c}}=10.107>2.57$ : la constante est significative pour un risque de première espèce de 1 %
* La probabilité critique est inférieure à 1\% pour la constante.

2.On vérifie la significativité celle des coefficients de l'AR(3) :

$\left\lbrace\begin{array}{l}
H_{0}\,:\,\phi_{i}=0\\
H_{a}\,:\,\phi_{i} \neq 0
\end{array} \right.$ 

*    $\vert t_{\hat{\phi}_{1}} \vert > 2.57$ et $\vert t_{\hat{\phi}_{3}}\vert> 2.57$ : les coefficients de retards 1 et 3 de l'AR sont significativement différents de 0 pour un risque de première espèce de 1\%.
*   Les probabilités critiques pour $\hat{\phi}_{1}$ et $\hat{\phi}_{3}$ sont inférieures à 1\% : on rejette l'hypothèse nulle de non-significativité pour un risque de première espèce de 1\%.
*  $\vert t_{\hat{\phi}_{2}} \vert =1.33 < 1.96$ : le coefficient du second retard de l'AR(3) n'est pas significativement différent de 0
*  La probabilité critique de $\hat{\phi}_{2}$ est égale à 21.1 % : ce coefficient n'est pas significatif pour un risque de 5%. 
* Le coefficient $\hat{\phi}_{3}$ du dernier retard est significativement différent de 0 : il n'y a pas de retard superflu.


2. Calcul des tstat directement

```{r}
AR3$tstat<-AR3$coef/sqrt(diag(AR3$var.coef))# tstat of the ARMA coefficient
AR3$tstat
```

#### Estimation de spécifications alternatives 

On estime un AR(4) pour vérifier s'il n'est pas nécessaire d'ajouter des retards supplémentaires

```{r}
AR4<-Arima(y = df_u_19T1$taux[1:100],order=c(4,0,0))
summary(AR4)
coeftest(AR4)
```

**Commentaires**

* On voit que le coefficient du quatrième retard ar4 n'est pas significativement différent de 0 $\vert t_{\hat{\phi}_{4}}\vert=0.693<1.96$


On estime un ARMA(3,1) pour vérifier s'il n'est pas nécessaire d'ajouter des retards MA supplémentaires 

```{r}
ARMA31<-Arima(y = df_u_19T1$taux[1:100],order=c(3,0,1))
summary(ARMA31)
coeftest(ARMA31)
```
**Commentaires**

* On voit que le coefficient du premier retard ma1 n'est pas significativement différent de 0 $\vert t_{\hat{\phi}_{4}}\vert=0.7752<1.96$. Il n'est pas nécessaire d'ajouter un retard MA.



#### Interprétation des coefficients

Dans la présentation de l'estimation, la constante reportée représente l'espérance de la série. Dans notre exemple les résultats peuvent s'écrire comme :
$$
\left\lbrace 
\begin{array}{l}
US\_taux_{t} = 6.9381+\hat{u}_{t}\\
\hat{u}_{t}=1.4712\times \hat{u}_{t-1}-0.2221\times \hat{u}_{t-2}- 0.2827\times \hat{u}_{t-3}+\hat{\epsilon}_{t}
\end{array}
\right. 
$$
$6.9381=E(US\_taux_{t})$ et $\hat{u}_{t} = US\_taux_{t}- 6.9381$ est le taux de chômage corrigé de sa moyenne.


#### Analyse des racines


On représente les **inverses** des racines dans le cercle unitaire. La condition de stationnarité impose que ces inverses doivent être de module strictement inférieur à 1, c'est-à-dire dans le cercle unitaire. La condition de stationnarité est donc satisfaite. 
```{r}
autoplot(AR3,main="Racines inverse de l\'AR(3)")
```

## Tests sur les résidus

Les tests de vérification des résidus sont :

1. Le test d'absence d'autocorrélation de Ljung-Box,
2. Le test de normalité de Jarque et Bera,
3. Le test d'absence d'effet GARCH de Engle et Granger.

#### Tests d'absence d'autocorrélation des résidus

On teste l'absence d'autocorrélation des résidus jusqu'à l'ordre 10. Les hypothèses du test sont :
$$
\left\lbrace 
\begin{array}{l}
H_{0}\,:\,\rho(1)=\rho(2)=\ldots=\rho(10)=0\\
H_{a}\,:\,\exists i\in\left\lbrace 1,\cdots,10 \right\rbrace\,\mathrm{tel\,que}\,\rho(i)\neq 0
\end{array}
\right. 
$$


```{r}
test_resAR3<-checkresiduals(AR3,lag = 10)
```

**Commentaires**

1. L'autocorrélogramme montre que toutes les autocorrélations sont situées dans les bornes de l'intervalle de confiance à 95\% : aucune des autocorrélations n'est significativement différente de 0.

2. La statistique de test de Ljung-Box est égale à Q*(10) = 9.1111.
    + Sous $H_{0}$, LB\_stat suit une loi $\chi^{2}(10-3) = \chi^{2}(7)$.
    + Les seuils critiques sont donc tirés de la loi $\chi^{2}(7)$ :
        + $Q^{*}(10)_{0.90}=12.02$ pour un risque de première espèce de 10%
        + $Q^{*}(10)_{0.95}=14.07$ pour un risque de première espèce de 5%
        + $Q^{*}(10)_{0.99}=18.48$ pour un risque de première espèce de 1%
    + Q*(10) = 9.1111  est inférieure à ces seuils critiques. On ne rejette l'hypothèse nulle d'absence d'autocorrélation des résidus pour un risque de première espèce de 1%.
    + La probabilité critique est égale à $0.2665$. Elle est supérieure aux risques de première espèce habituel (1%, 5% et 10%). On ne rejette pas l'hypothèse nulle d'absence d'autocorrélation des résidus.


#### Test de l'hypothèse de normalité des résidus

On représente :

1. l'histogramme des résidus (rectangles gris )
2. l'histogramme d'une loi normale avec la même moyenne et le même écart type que celui des résidu (trait rouge)

On peut remarquer que l'h
```{r}
gghistogram(AR3$residuals, add.normal = TRUE)
```

On peut voir qu'il existe des différences entre les deux histogrammes ce qui permet de supposer que les résidus ne suivent pas une loi normale.


#### Test de l'hypothèse en normalité de Jarque and Bera 

```{r}
library(moments)
sprintf("Skewness des residus : %f",skewness(AR3$residuals))
sprintf("kurtosis des residus : %f",kurtosis(AR3$residuals))
```
* Le skewness estimé est positif : on a une asymétrie du côté des valeurs positives
* Le kurtosis estimé est supérieur à 3 : c'est l'indice d'une loi non gaussienne


```{r}
library(tseries)
jarque.bera.test(AR3$residuals)
```
Les hypothèses du test sont :
$$
\left\lbrace 
\begin{array}{l}
H_ {0}:\,S(X)=0\;et\;K(X)=3\\
H_ {1}:\,S(X)\neq 0\; ou\;K(X)\neq 3
\end{array}
\right. 
$$
* La statistique du test de Jarque et Bera est égale à  est JB\_stat = 11.338.
* Sous $H_{0}$, LB\_stat suit une loi $\chi^{2}(2)$.
* Les seuils critiques sont :
    + $\chi^{2}_{0.90}(2)=4.61$ pour un risque de première espèce de 10%
    + $\chi^{2}_{0.95}(2)=5.99$ pour un risque de première espèce de 5%
    + $\chi^{2}_{0.99}(2)=9.21$ pour un risque de première espèce de 1%
    
* JB\_stat = 11.338 est supérieure aux seuils critiques pour des risques de première espèce de 1%, 5% et 10\% : on rejette l'hypothèse d'une loi normale pour les résidus.
* La probabilité critique est égale à $0.001$ : on rejette l'hypothèse nulle de loi normale à 10%, 5\% et 1%.
* Le rejet de l'hypothèse de normalité peut s'expliquer par les pics du taux de chômage observés en 2010(grande récession). Le valeur du taux de chômage est alors beaucoup plus élevée que lors de période précédente. Ceci peut expliquer le kurtosis supérieur à 3


#### Test de Engle Granger d'absence d'effet ARCH

On utilise la fonction ArchTest de la library FinTS (il existe d'autres fonctions dans les packages ATSA et NortsTest notamment)

On teste l'hypothèse d'absence d'effet ARCH avec quatre retards. La régression estimée est :
$$
\hat{\epsilon}_{t}^{2}= \gamma_{0} + \gamma_{1} \hat{\epsilon}_{t-1}^{2} +\gamma_{2} \hat{\epsilon}_{t-2}^{2} +\gamma_{3} \hat{\epsilon}_{t-3}^{2} +\gamma_{4} \hat{\epsilon}_{t-4}^{2} + v_{t}
$$


Les hypothèses du test sont :
$$
\left\{
\begin{array}{ll}
H_{0} : \gamma_{1}=...=\gamma_{4}=0 \Rightarrow \,pas\,d'effet\,ARCH\\
H_{a} : \gamma_{1}\neq 0\, ou \,... \gamma_{4} \neq 0 \Rightarrow effet\,ARCH
\end{array}
\right.
$$
Les résultats du test d'absence d'effet ARCH figurent ci-dessous :

```{r}
library(FinTS)
ArchTest(AR3$residuals,lags=4,demean = FALSE)
```

* La statistique de test est ARCH\_LM = 25.704.
* Sous $H_{0}$, LB\_stat suit une loi $\chi^{2}(2)$. Les seuils critiques sont :
    + $\chi^{2}_{0.90}(2)=4.61$ pour un risque de première espèce de 10%
    + $\chi^{2}_{0.95}(2)=5.99$ pour un risque de première espèce de 5%
    + $\chi^{2}_{0.99}(2)=9.21$ pour un risque de première espèce de 1%
*  ARCH\_LM = 23.13 est supérieure aux seuils critiques : on ne rejette l'hypothèse nulle d'absence d'effet ARCH pour les résidus pour les risques de première espèce habituel (1%, 5% et 10%)
*  La probabilité critique est égale à $0.000$ : on rejette l'hypothèse nulle d'absence d'effet ARCH 
* La hausse exceptionnelle du taux de chômage en 2010 explique ce rejet de l'hypothèse d'homoscédasticité.


### Prévision 

#### Calcul des prévision et représentation graphique
On calcule les prévisions pour l'année 2019 (soit à l'horizon h=4) ainsi que leur intervalle de confinance à 95 %. 

```{r}
library(forecast)
forecast_AR3<-forecast(AR3,h=4,level=95)
forecast_AR3
autoplot(forecast_AR3)
```

$$
\left\lbrace 
\begin{array}{l}
US\_taux_{t} = 6.9381+\hat{u}_{t}\\
\hat{u}_{t}=1.4712\times \hat{u}_{t-1}-0.2221\times \hat{u}_{t-2}- 0.2827\times \hat{u}_{t-3}+\hat{\epsilon}_{t}
\end{array}
\right.
$$
$$
\begin{aligned}
\Rightarrow US\_taux_{t} - 6.9381 &= 1.4712\times(US\_taux_{t-1} - 6.9381)-0.2221\times (US\_taux_{t-2} - 6.9381)- 0.2827\times(US\_taux_{t-3} - 6.9381)+\hat{\epsilon}_{t}\\
\Rightarrow US\_taux_{t} &  = (1 - 1.4712 + 0.2221 + 0.2827)\times 6.9381+ 1.4712\times US\_taux_{t-1}-0.2221\times US\_taux_{t-2} - 0.2827\times US\_taux_{t-3} +\hat{\epsilon}_{t}\\
\Rightarrow US\_taux_{t} &= 0.2331+ 1.4712\times US\_taux_{t-1}-0.2221\times US\_taux_{t-2} - 0.2827\times US\_taux_{t-3} +\hat{\epsilon}_{t}
\end{aligned}
$$

**Calcul de la prévision pour 2019:T1 (h=1)**

Les prévisions du de  taux de chômage vont dépendre uniquement des trois dernières observations passées
```{r}
tail(forecast_AR3$mean,n=4)
tail(df_u_19T1[98:100,])
```


$$
\begin{aligned}
US\_taux_{2019:T1}^{a} &= 0.2331+ 1.4712\times US\_taux_{2018:T4}-0.2221\times US\_taux_{2018:T3} - 0.2827\times US\_taux_{2018:T2}\\
\Rightarrow US\_taux_{2019:T1}^{a}&= 0.2331+ 1.4712\times 4.8-0.2221\times 4.7 - 0.2827\times 4.8 = 4.8940 
\end{aligned}
$$

**Calcul de la prévision pour 2019:T2 (h=2)**
$$
\begin{aligned}
US\_taux_{2019:T2}^{a}& = 0.2331+ 1.4712\times US\_taux_{2019:T1}^{a}-0.2221\times US\_taux_{2018:T4} - 0.2827\times US\_taux_{2018:T3}\\
\Rightarrow US\_taux_{2019:T1}^{a}&= 0.2331+ 1.4712\times 4.8940-0.2221\times 4.8 - 0.2827\times 4.7 = 5.0384
\end{aligned}
$$

**Calcul de la prévision pour 2019:T3 (h=3)**

$$
\begin{aligned}
US\_taux_{2019:T3}^{a} &= 0.2331+ 1.4712\times US\_taux_{2019:T2}^{a}-0.2221\times US\_taux_{2019:T1}^{a} - 0.2827\times US\_taux_{2018:T4}\\
\Rightarrow US\_taux_{2019:T1}^{a}&= 0.2331+ 1.4712\times 5.0384-0.2221\times 4.8940 - 0.2827\times 4.8 = 5.2016
\end{aligned}
$$
**Calcul de la prévision pour 2019:T4 (h=4)**

$$
\begin{aligned}
US\_taux_{2019:T4}^{a}& = 0.2331+ 1.4712\times US\_taux_{2019:T3}^{a}-0.2221\times US\_taux_{2019:T2}^{a} - 0.2827\times US\_taux_{2019:T1}^{a}\\
\Rightarrow US\_taux_{2019:T1}^{a}&= 0.2331+ 1.4712\times 5.2016-0.2221\times 5.0384 - 0.2827\times 4.8940 = 5.3831
\end{aligned}
$$



```{r}

df_forc<-data.frame(df_u_19T1$date[101:104],df_u_19T1$taux[101:104],as.matrix(forecast_AR3$mean))

colnames(df_forc)<-c("date","observé","prévue")


data_F_melt=melt(as.data.frame(df_forc),id.vars = 1)

p_g_us<-ggplot(data=data_F_melt,aes(x=date))+geom_line(aes(x=date,y=value,color= variable))
p_g_us+labs(y="taux en pourcentage",title = "Valeur observée et valeur ajustée du taux de chômage",
            subtitle = "2019:T1 à T4")

```


Le modèle a tendance à surestimer l'évolution future du taux de chômage par rapport à ses valeurs observées.


#### Evaluation des la qualité des prévision hors échantillon


```{r}
library(Metrics)
sprintf("MAE out of sample %f",mae(df_forc$observé,df_forc$prévue))
sprintf("RMSE out of sample %f",rmse(df_forc$observé,df_forc$prévue))
```
La MAE et la RMSE hors échantillon sont plus élevées que leur valeur dans l'ensemble d'estimation (training set). Les valeurs hors échantillon permette d'évaluer plus précisément la qualité prédictive du modèle estimé.



### Ajout de variables indicatrices pour l'année 2008

#### Estimation du modèle AR(3) et tests de spécification

Le [NBER](https://www.nber.org/research/data/us-business-cycle-expansions-and-contractions) date la ``Grande Récession'' de 2007:T4 à 2009:T2. On définit des variables indicatrices individuelles pour les trimestres de la période et on les ajoute au modèle AR(3).

```{r}
df_u_19T1$du_07T4<-as.numeric(df_u_19T1$date=="2007-10-01")
df_u_19T1$du_08T1<-as.numeric(df_u_19T1$date=="2008-01-01")
df_u_19T1$du_08T2<-as.numeric(df_u_19T1$date=="2008-04-01")
df_u_19T1$du_08T3<-as.numeric(df_u_19T1$date=="2008-07-01")
df_u_19T1$du_08T4<-as.numeric(df_u_19T1$date=="2008-10-01")
df_u_19T1$du_09T1<-as.numeric(df_u_19T1$date=="2009-01-01")
df_u_19T1$du_09T2<-as.numeric(df_u_19T1$date=="2009-04-01")
AR3_DU<-Arima(y = df_u_19T1$taux[1:100],order=c(3,0,0),xreg=as.matrix(df_u_19T1[1:100,c("du_07T4","du_08T1","du_08T2","du_08T3","du_08T4")]))
summary(AR3_DU) 
```


```{r}
library(lmtest)
coeftest(AR3_DU)
```











**Commentaires**

* Le coefficient de l'AR(2) n'est plus significatif. La constante et les coefficients de l'AR(1) et l'AR(3) sont significatifs à 1% et leur ordre de grandeur est proche de celui du modèle sans indicatrices.
* Toutes les indicatrices sont significatives au moins à 10% (du_07T4), à 5% (du_08T1) ou à 1%
* Le critère AIC est plus petit que pour la régression sans indicatrice, le critère BIC est plus élevé.
* La RMSE et la MAE reportée sont les valeurs pour l'échantillon d'estimation ("Training set"). On va comparer les comparer à leur valeur hors échantillon ("out-of-sample") en calculant des prévisions pour des observations n'ayant pas été utilisées pour estimer le modèle.


On représente ci-dessous les valeurs observées et les valeurs prédites par le modèle. On voit que les deux courbes sont assez proches, ce qui permet de penser que la qualité de l'ajustement n'est pas trop mauvaise. 

```{r}
library(ggplot2)
library(reshape2)

df_plot<-data.frame(df_u_19T1$date[1:100],df_u_19T1$taux[1:100],F=as.matrix(AR3_DU$fitted))
colnames(df_plot)<-c("date","observé","ajusté")

data_melt=melt(as.data.frame(df_plot),id.vars = 1)

p_F_us<-ggplot(data=data_melt,aes(x=date))+geom_line(aes(x=date,y=value,color= variable))
p_F_us+labs(y="taux en pourcentage",title = "Valeur observée et valeur ajustée du taux de chômage",
            subtitle = "1994:T1 à 2018:T4")
```





```{r}
library(tseries)
library(FinTS)
plot(AR3_DU$residuals)
```


```{r}
jarque.bera.test(AR3_DU$residuals)
ArchTest(AR3_DU$residuals,lags=4,demean = FALSE)
```

**Commentaires**
* On ne rejette pas l'hypothèse nulle de loi normale (test de Jarque et Bera) pour un risque de première espèce de 1%.
* On ne rejette pas l'hypothèse nulle d'absence d'effet ARCH (test de Engle-Granger) pour un risque de première espèce de 1%.
* Les variables indicatrices pour la période de la Grande Récession permettent d'obtenir des résidus compatibles avec la loi normale et sans effet ARCH.


#### Prévision 

```{r}
library(forecast)
forecast_DU<-forecast(AR3_DU,h=4, xreg = as.matrix(df_u_19T1[101:104,c("du_07T4","du_08T1","du_08T2","du_08T3","du_08T4")]))
autoplot(forecast_DU)
```

Le calcul des prévisions se fait exactement comme avec le modèle sans indicatrices. Dans notre cas, les indicatrices sont définies par rapport à des dates données. Leurs valeurs futures sont égales à 0.


```{r}

df_forc_DU<-data.frame(df_u_19T1$date[101:104],df_u_19T1$taux[101:104],as.matrix(forecast_DU$mean))

colnames(df_forc_DU)<-c("date","observé","prévue")


data_F_DU_melt=melt(as.data.frame(df_forc_DU),id.vars = 1)

p_g_us<-ggplot(data=data_F_DU_melt,aes(x=date))+geom_line(aes(x=date,y=value,color= variable))
p_g_us+labs(y="taux en pourcentage",title = "Valeur observée et valeur ajustée du taux de chômage",
            subtitle = "2019:T1 à T4")

```


Comme avec le modèle précédent, les valeurs prévues sont plus élevées que les valeurs observées.


```{r}
library(Metrics)
sprintf("MAE out of sample %f",mae(df_forc_DU$observé,df_forc_DU$prévue))
sprintf("RMSE out of sample %f",rmse(df_forc_DU$observé,df_forc_DU$prévue))
```

La RMSE et la MAE sont plus faibles pour les prévisions obtenues du modèle sans indicatrices. Les indicatrices ont donc améliorer les capacités prédictives du modèles même si l'évolution future du taux de chômage est surestimée. 



###  Estimation sur la totalité de l'échantillon 1994:T1 à 2023:T1

L'échantillon total est caractérisé notamment par la hausse brutale et temporaite du taux de chômage pendant l'épidémie de covid. Après cette période, le taux de chômage revient à des niveaux comparables à ses valeurs passées. On devra sans doute introduire des variables indicatrices pour prendre en compte les effets de l'épidémie de covid.


#### Première estimation

On commence par estimer l'autocorrélogramme.

```{r}
acf(x = df_u$taux)
pacf(x = df_u$taux)
```
**Commentaires**

* Pas de modification des autocorrélations par rapport à la période 1994:T1-2019:T4
* Seule l'autocorrélation partielle à l'ordre 1 est maintenant significitive.


On estime dans un premier temps un AR(1)

```{r}
AR1_C<-Arima(y = df_u$taux[1:114],order=c(1,0,0))
summary(AR1_C)
coeftest(AR1_C)
```

**Commentaires**
*   La constante et le coefficient ar1 sont significativement différents de 0


```{r}
checkresiduals(AR1_C)
```

**Commentaires**
*    Le test de Ljung-Box avec 10 retards ne rejette pas l'hypothèse de nullité des 10 premières autocorrélations.

```{r}
jarque.bera.test(AR1_C$residuals)
ArchTest(AR1_C$residuals,lags=4,demean = FALSE)
```

**Commentaires**
* Le test de Jarque et Bera rejette l'hypothèse nulle de normalité des résidus
* Le test ARCH rejette l'hypothèse nulle d'absence d'effet ARCH.

### Ajout de variables indicatrices

On va essayer d'améliorer le modèle en ajoutant des variables indicatrices :

* pour la crise de 2007-2008 comme précédemment
* pour la crise du covid 

L'ajout de ces indicatrices va nous obliger à augmenter le nombre de retards et de passer un AR(2). En effet, l'AR(1) avec les indicatrices présente de l'autocorrélation des résidus. 

```{r}
df_u$du_07T4<-as.numeric(df_u$date=="2007-10-01")
df_u$du_08T1<-as.numeric(df_u$date=="2008-01-01")
df_u$du_08T2<-as.numeric(df_u$date=="2008-04-01")
df_u$du_08T3<-as.numeric(df_u$date=="2008-07-01")
df_u$du_08T4<-as.numeric(df_u$date=="2008-10-01")
df_u$du_09T1<-as.numeric(df_u$date=="2009-01-01")
df_u$du_09T2<-as.numeric(df_u$date=="2009-04-01")


df_u$du_20T1<-as.numeric(df_u$date=="2020-01-01")
df_u$du_20T2<-as.numeric(df_u$date=="2020-04-01")
df_u$du_20T3<-as.numeric(df_u$date=="2020-07-01")
df_u$du_20T4<-as.numeric(df_u$date=="2020-10-01")


AR2_DU_C<-Arima(y = df_u$taux[1:114],order=c(2,0,0),xreg=as.matrix(df_u[1:114,c("du_09T1","du_09T2","du_20T2","du_20T3","du_20T4")]))
summary(AR2_DU_C) 
```
```{r}
coeftest(AR2_DU_C)
```

**Commentaires**

* Les trois indicatrices relatives à l'année 2020T2, T3 et T4 sont significatives et positives pour un risque de première espèce de 1%
* Seules les indicatrices pour 2009:T1 et 2009:T2 s'avèrent significatives quand on prend en compte la totalité des observations pour un risque de première espèce de 1%.
* La constante et les coefficients ar1 et ar2 sont significatifs pour un risque de première espèce de 1% 
* Les critères AIC et BIC sont nettement plus petits que pour le AR(1) sant indicatrices.



```{r}
checkresiduals(AR2_DU_C)
```

**Commentaires**

*  Le test de Ljung-Box ne rejette pas l'hypothèse d'absence d'autocorrélation des résidus.


```{r}
jarque.bera.test(AR2_DU_C$residuals)
ArchTest(AR2_DU_C$residuals,lags=4,demean = FALSE)
```

**Commentaires**

*     Le test de Jarque et Bera ne rejette plus l'hypothèse de normalités des résidus
*     Le test ARCH rejette encore l'hypothèse nulle d'absence d'effet ARCH. Malgrè l'introduction de variables indicatrices, la variance du taux de chômage dépend du temps.


### Prévision

On calcule les prévisions à partir du modèle AR(2) avec les indicatrices

```{r}
library(forecast)
forecast_DU_C<-forecast(AR3_DU,h=4, xreg = as.matrix(df_u[114:118,c("du_09T1","du_09T2","du_20T2","du_20T3","du_20T4")]))
autoplot(forecast_DU_C)
```
```{r}

df_forc_DU<-data.frame(df_u$date[114:118],df_u$taux[114:118],as.matrix(forecast_DU_C$mean))

colnames(df_forc_DU)<-c("date","observé","prévue")


data_F_DU_melt=melt(as.data.frame(df_forc_DU),id.vars = 1)

p_g_us<-ggplot(data=data_F_DU_melt,aes(x=date))+geom_line(aes(x=date,y=value,color= variable))
p_g_us+labs(y="taux en pourcentage",title = "Valeur observée et valeur ajustée du taux de chômage",
            subtitle = "2019:T1 à T4")

```


Le modèle surestime l'évolution future du taux de chômage.


```{r}
library(Metrics)
sprintf("MAE out of sample %f",mae(df_forc_DU$observé,df_forc_DU$prévue))
sprintf("RMSE out of sample %f",rmse(df_forc_DU$observé,df_forc_DU$prévue))
```

La MAE et la RMSE du modèle out-of-sample sont plus élevés que leur valeur "in sample" (RMSE et MAE in training)


#### Méthodes d'estimations alternatives

On présente quelques fonctions et packages alternatifs.

##### auto.arima

Dans la librairie forecast, la fonction auto.arima permet de sélectionner le meilleur modèle en spcifiant le nombre de retards maximum pour la composante AR (max.p), la composante MA (max.q),
le nombre de différenciation de la série (max.d). Ici on a contraint max.d=0 mais sa valeur par défaut est max.d=1


```{r}
best_ARMA <-auto.arima(df_u_19T1$taux,max.p = 4,max.q=4,max.d=0)
class(best_ARMA)
summary(best_ARMA)
arimaorder(best_ARMA)
```



##### Librairie tseries
```{r}
library(tseries)
order=c(3,0) # order of the ARMA model c(ARlag,MAlag)
AR2_b = arma(x=df_u_19T1$taux,order=c(2,0),include.intercept=TRUE)
summary(AR2_b) # to display the results
```


##### Librairie FinTS

```{r}
library(FinTS)
AR2_fin = ARIMA(x = df_u_19T1$taux,order=c(3,0,0),type=c("Ljung-Box"))
summary(AR2_fin)
AR2_fin$Box.test
tsdiag(AR2_fin)

#plotArmaTrueacf(AR2_fin)
#ArchTest(AR2_fin$residuals,lags = 2)
```



